{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06793bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# ### 1. Importing Libraries\n",
    "# These are the libraries we will be using for data manipulation, visualization, and machine learning.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, PrecisionRecallDisplay\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "\n",
    "# ### 2. Loading the Data\n",
    "print(\"Loading the dataset...\")\n",
    "try:\n",
    "    df = pd.read_csv(r'../data/creditcard.csv') \n",
    "    display(df.head())\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: The dataset file was not found. Please check the file path.\")\n",
    "    print(\"Please ensure you have downloaded the dataset and placed it in the correct directory.\")\n",
    "\n",
    "print(f\"Dataset contains {df.shape[0]} rows and {df.shape[1]} columns.\")\n",
    "display(df.describe())\n",
    "\n",
    "# proportion of transactions that are fraud\n",
    "fraud = df[df['Class'] == 1]\n",
    "valid = df[df['Class'] == 0]\n",
    "\n",
    "fractional_value = len(fraud)/(len(valid))\n",
    "print(\"Fractional value:\", fractional_value)\n",
    "\n",
    "print(\"Fraud Cases detected: {}\".format(len(df[df['Class'] == 1])))\n",
    "print(\"Valid Transactions: {}\".format(len(df[df['Class'] == 0])))\n",
    "print(\"Total Transactions: {}\".format(len(df)))\n",
    "\n",
    "\n",
    "# === Missing Values & Duplicates ===\n",
    "print(\"\\nMissing values per column:\\n\", df.isna().sum())\n",
    "print(\"Duplicate rows:\", df.duplicated().sum())\n",
    "df = df.drop_duplicates()\n",
    "print(\"After dropping duplicates:\", df.shape)\n",
    "if df.isna().sum().sum() > 0:\n",
    "    df = df.dropna()\n",
    "    print(\"After dropping NA:\", df.shape)\n",
    "\n",
    "\n",
    "# === Dataset Info ===\n",
    "print(\"\\nDataset info:\")\n",
    "df.info()\n",
    "\n",
    "\n",
    "# === Class Balance ===\n",
    "fraud_counts = df['Class'].value_counts()\n",
    "transaction = len(df)\n",
    "\n",
    "fraud_percentage = (fraud_counts.get(1, 0) / transaction) * 100\n",
    "print(f\"Percentage of fraudulent transactions: {fraud_percentage:.4f}%\")\n",
    "print(\"This confirms a severe class imbalance, which must be addressed in our modeling.\")\n",
    "\n",
    "sns.countplot(x='Class', data=df)\n",
    "plt.title(\"Class Distribution (0: No Fraud, 1: Fraud)\")\n",
    "plt.show()\n",
    "print(df[\"Class\"].value_counts(normalize=True))\n",
    "\n",
    "\n",
    "# === Feature Distributions ===\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.histplot(df['Amount'], bins=50, kde=True)\n",
    "plt.title(\"Transaction Amount Distribution\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.histplot(df['Time'], bins=50, kde=False)\n",
    "plt.title(\"Transaction Time Distribution\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# === Correlation Heatmap ===\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.heatmap(df.corr(), cmap=\"coolwarm\", annot=False)\n",
    "plt.title(\"Correlation Heatmap of Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4811311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...existing code...\n",
    "# --- C-2: Baseline Model — Logistic Regression (train & evaluate on original imbalanced data) ---\n",
    "\n",
    "# Ensure X, y and a train/test split exist; create if missing\n",
    "if 'X_train' not in globals() or 'X_test' not in globals():\n",
    "    features = [c for c in df.columns if c not in ['Class', 'Amount', 'Time']]\n",
    "    X = df[features]\n",
    "    y = df['Class']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, stratify=y, random_state=42\n",
    "    )\n",
    "    print(\"Created train/test split:\", X_train.shape, X_test.shape)\n",
    "else:\n",
    "    print(\"Using existing train/test split.\")\n",
    "\n",
    "# Train baseline logistic regression (no resampling here)\n",
    "baseline_lr = LogisticRegression(max_iter=1000,solver=\"saga\", random_state=42)\n",
    "baseline_lr.fit(X_train, y_train)\n",
    "\n",
    "# Predictions & probabilities\n",
    "y_pred = baseline_lr.predict(X_test)\n",
    "y_prob = baseline_lr.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "print(\"\\n=== Baseline Logistic Regression Evaluation ===\")\n",
    "print(classification_report(y_test, y_pred, digits=4))\n",
    "\n",
    "roc_auc = roc_auc_score(y_test, y_prob)\n",
    "print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "# Confusion matrix plot\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=[\"Valid (0)\",\"Fraud (1)\"], yticklabels=[\"Valid (0)\",\"Fraud (1)\"])\n",
    "plt.ylabel(\"True label\")\n",
    "plt.xlabel(\"Predicted label\")\n",
    "plt.title(\"Confusion Matrix — Baseline LR\")\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(fpr, tpr, label=f'ROC AUC = {roc_auc:.4f}')\n",
    "plt.plot([0,1],[0,1], 'k--', alpha=0.6)\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve — Baseline LR\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Precision-Recall curve and average precision\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_prob)\n",
    "avg_prec = average_precision_score(y_test, y_prob)\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(recall, precision, label=f'AP = {avg_prec:.4f}')\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve — Baseline LR\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNotes:\")\n",
    "print(\"- This baseline is trained on the original imbalanced data (no SMOTE/weighting).\")\n",
    "print(\"- Use these results as a reference when you apply SMOTE or class-weighting in later tasks (C-3, C-4).\")\n",
    "\n",
    "print(\"          ---Task C-2 complete---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f22a014",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "print(\"\\n=== C-3: Logistic Regression with SMOTE ===\")\n",
    "\n",
    "# Apply SMOTE only on the training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_sm, y_train_sm = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Train model\n",
    "lr_smote = LogisticRegression(max_iter=2000, random_state=42)\n",
    "lr_smote.fit(X_train_sm, y_train_sm)\n",
    "\n",
    "# Predict on original test set\n",
    "y_pred_sm = lr_smote.predict(X_test)\n",
    "y_prob_sm = lr_smote.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "print(classification_report(y_test, y_pred_sm))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_prob_sm))\n",
    "print(\"Average Precision:\", average_precision_score(y_test, y_prob_sm))\n",
    "\n",
    "# Optional: Plot PR curve\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_prob_sm)\n",
    "plt.plot(recall, precision)\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve (SMOTE)\")\n",
    "plt.show()\n",
    "\n",
    "print(\"          ---Task C-3 complete---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db85eb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== C-4: Random Forest with SMOTE ===\")\n",
    "\n",
    "# Balance training set\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_sm, y_train_sm = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Train Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1, class_weight=\"balanced\")\n",
    "rf.fit(X_train_sm, y_train_sm)\n",
    "\n",
    "# Predictions\n",
    "y_pred = rf.predict(X_test)\n",
    "y_prob = rf.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Metrics\n",
    "print(classification_report(y_test, y_pred, digits=4))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_prob))\n",
    "print(\"Average Precision:\", average_precision_score(y_test, y_prob))\n",
    "\n",
    "# Curves\n",
    "fpr,tpr,_=roc_curve(y_test,y_prob)\n",
    "plt.plot(fpr,tpr,label=f'ROC AUC={roc_auc_score(y_test,y_prob):.4f}')\n",
    "plt.plot([0,1],[0,1],'k--');plt.legend();plt.show()\n",
    "\n",
    "precision,recall,_=precision_recall_curve(y_test,y_prob)\n",
    "plt.plot(recall,precision,label=f'AP={average_precision_score(y_test,y_prob):.4f}')\n",
    "plt.legend();plt.show()\n",
    "\n",
    "print(\"          ---Task C-4 complete---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b676ab64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === C-5: Final Performance Evaluation ===\n",
    "print(\"\\n=== C-5: Final Model Evaluation (Random Forest vs Baseline) ===\")\n",
    "\n",
    "# Baseline metrics (from C-2)\n",
    "baseline_report = classification_report(y_test, y_pred, digits=4, output_dict=True)\n",
    "\n",
    "# Random Forest metrics (from C-4)\n",
    "rf_report = classification_report(y_test, y_pred, digits=4, output_dict=True)\n",
    "\n",
    "# Compare recall for fraud class (label = 1)\n",
    "print(f\"Baseline Recall (Fraud=1): {baseline_report['1']['recall']:.4f}\")\n",
    "print(f\"Random Forest Recall (Fraud=1): {rf_report['1']['recall']:.4f}\")\n",
    "print(f\"Improvement: {rf_report['1']['recall'] - baseline_report['1']['recall']:.4f}\")\n",
    "\n",
    "print(\"\\nFull Report — Random Forest:\")\n",
    "print(classification_report(y_test, y_pred, digits=4))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_prob))\n",
    "print(\"Average Precision:\", average_precision_score(y_test, y_prob))\n",
    "\n",
    "\n",
    "# === C-6: Save Final Model ===\n",
    "print(\"\\n=== C-6: Saving Final Trained Model ===\")\n",
    "joblib.dump(rf, \"final_rf_model.pkl\")\n",
    "print(\"Model saved as 'final_rf_model.pkl'\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
